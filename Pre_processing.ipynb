{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, random\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "from scipy import ndimage, misc\n",
    "\n",
    "# directory: ./data/*/*.nii.gz\n",
    "# there are different modalities that should be taken care of\n",
    "\n",
    "\n",
    "class Data:\n",
    "    def __init__(self, moda):\n",
    "        self.moda = moda\n",
    "        # [\"FLAIR_preprocessed\", \"T1_preprocessed\", \"T2_preprocessed\"]\n",
    "        self.input = {m : defaultdict(list) for m in self.moda}\n",
    "        self.target = defaultdict(list)\n",
    "        self.kfold = None\n",
    "        self.batch_size = None\n",
    "        self.patch_size = None\n",
    "        self.patch_gap = None\n",
    "        self.patch_index = defaultdict(list)\n",
    "        self.valid_index = {}\n",
    "        # pre-set seed so that reconstruct can retrieve valid_index for final result\n",
    "        random.seed(3000)\n",
    "        \n",
    "    def fetch_raw_data_from_file(self, modality, file_name):\n",
    "        def fetch_file(modality):\n",
    "            # could be \"Consensus\", \"FLAIR_preprocessed\"\n",
    "            data = []\n",
    "            root, sub_dir, _ = next(os.walk(os.getcwd() + '/data/'))\n",
    "            for sub in sub_dir:\n",
    "                data.append(os.path.join(root, sub + '/' + modality + '.nii.gz'))\n",
    "            return data\n",
    "        file = fetch_file(modality)\n",
    "        raw_data = defaultdict(list)\n",
    "        for i in range(len(file)):\n",
    "            image = nib.load(file[i])\n",
    "            raw_data[image.shape].append(image.get_fdata())\n",
    "        with h5py.File(file_name, 'w') as f:\n",
    "            for i in raw_data:\n",
    "                f.create_dataset(str(i), data=raw_data[i])\n",
    "        return raw_data\n",
    "    \n",
    "    def zero_pad(self, image, div=(32, 32, 32)):\n",
    "        pad_size = [0, 0, 0]\n",
    "        pad = False\n",
    "        for i in range(len(image.shape)):\n",
    "            remain = image.shape[i] % div[i]\n",
    "            if remain != 0:\n",
    "                pad = True\n",
    "                pad_size[i] = (image.shape[i] // div[i] + 1) * div[i] - image.shape[i]\n",
    "        if pad:\n",
    "            # deal with odd number of padding\n",
    "            pad0 = (pad_size[0]//2, pad_size[0] - pad_size[0]//2)\n",
    "            pad1 = (pad_size[1]//2, pad_size[1] - pad_size[1]//2)\n",
    "            pad2 = (pad_size[2]//2, pad_size[2] - pad_size[2]//2)\n",
    "            # https://stackoverflow.com/questions/50008587/zero-padding-a-3d-numpy-array\n",
    "            return np.pad(image, (pad0, pad1, pad2), 'constant')\n",
    "        else:\n",
    "            return image\n",
    "    \n",
    "    def check_file_exist(self, path, pattern):\n",
    "        for m in self.moda:\n",
    "            if not os.path.isfile(path + pattern + m + '.h5'):\n",
    "                return False\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def fetch_target_data(self, target_path):\n",
    "        self.fetch_raw_data_from_file('Consensus', 'target_data.h5')\n",
    "        return self.load_target_data(target_path)\n",
    "    \n",
    "    def fetch_raw_data(self, raw_path):\n",
    "        for m in self.moda:\n",
    "            # different modality: same order of images\n",
    "            self.fetch_raw_data_from_file(m, raw_path + 'raw_data_' + m + '.h5')\n",
    "        return self.load_raw_data(raw_path)\n",
    "    \n",
    "    def load_target_data(self, target_path):\n",
    "        target_file = h5py.File(target_path + 'target_data.h5', 'r')\n",
    "        target_data = defaultdict(list)\n",
    "        for i in target_file.keys():\n",
    "            target_data[i] = target_file[i]\n",
    "        return target_data, target_file\n",
    "    \n",
    "    def load_raw_data(self, raw_path):\n",
    "        raw_file = []\n",
    "        raw_data = {m : defaultdict(list) for m in self.moda}\n",
    "        for m in self.moda:\n",
    "            data = defaultdict(list)\n",
    "            file = h5py.File(raw_path + 'raw_data_' + m + '.h5', 'r') \n",
    "            raw_file.append(file)\n",
    "            for i in file.keys():\n",
    "                # to get the matrix: self.data[i][:]\n",
    "                # d.data[i][j][0], d.data[i][j][1]\n",
    "                data[i] = file[i]\n",
    "            raw_data[m] = data\n",
    "        return raw_data, raw_file\n",
    "    \n",
    "        # raw_data:\n",
    "        # {'FLAIR_preprocessed': defaultdict(list,\n",
    "        #               {'(128, 224, 256)': <HDF5 dataset \"(128, 224, 256)\": shape (5, 128, 224, 256), type \"<f8\">,\n",
    "        #                '(144, 512, 512)': <HDF5 dataset \"(144, 512, 512)\": shape (5, 144, 512, 512), type \"<f8\">,\n",
    "        #                '(261, 336, 336)': <HDF5 dataset \"(261, 336, 336)\": shape (5, 261, 336, 336), type \"<f8\">}),\n",
    "        #   'T1_preprocessed': defaultdict(list,\n",
    "        #               {'(128, 224, 256)': <HDF5 dataset \"(128, 224, 256)\": shape (5, 128, 224, 256), type \"<f8\">,\n",
    "        #                '(144, 512, 512)': <HDF5 dataset \"(144, 512, 512)\": shape (5, 144, 512, 512), type \"<f8\">,\n",
    "        #                '(261, 336, 336)': <HDF5 dataset \"(261, 336, 336)\": shape (5, 261, 336, 336), type \"<f8\">}),\n",
    "        #   'T2_preprocessed': defaultdict(list,\n",
    "        #               {'(128, 224, 256)': <HDF5 dataset \"(128, 224, 256)\": shape (5, 128, 224, 256), type \"<f8\">,\n",
    "        #                '(144, 512, 512)': <HDF5 dataset \"(144, 512, 512)\": shape (5, 144, 512, 512), type \"<f8\">,\n",
    "        #                '(261, 336, 336)': <HDF5 dataset \"(261, 336, 336)\": shape (5, 261, 336, 336), type \"<f8\">})}\n",
    "        # raw_file:\n",
    "        #  [<HDF5 file \"raw_data_FLAIR_preprocessed.h5\" (mode r)>,\n",
    "        #   <HDF5 file \"raw_data_T1_preprocessed.h5\" (mode r)>,\n",
    "        #   <HDF5 file \"raw_data_T2_preprocessed.h5\" (mode r)>]\n",
    "\n",
    "    def pad_target_data(self, patch_size, pad_path, target_path):\n",
    "        # input\n",
    "        target_data = None\n",
    "        target_file = None\n",
    "        if os.path.isfile(target_path + \"target_data.h5\"):\n",
    "            target_data, target_file = self.load_target_data(target_path)\n",
    "        else:\n",
    "            target_data, target_file = self.fetch_target_data(target_path)\n",
    "        \n",
    "        pad_target_data = defaultdict(list)\n",
    "        for i in target_data.keys():\n",
    "            for j in range(target_data[i].shape[0]):\n",
    "                img = self.zero_pad(target_data[i][j], patch_size)\n",
    "                pad_target_data[img.shape].append(img)\n",
    "\n",
    "        target_file.close()\n",
    "\n",
    "        with h5py.File(pad_path + \"pad_target_data.h5\" , 'w') as f:\n",
    "            f.create_dataset(\"patch_size\", data=patch_size)\n",
    "            for i in pad_target_data:\n",
    "                f.create_dataset(str(i), data=pad_target_data[i])\n",
    "        \n",
    "        pad_file = h5py.File(pad_path + \"pad_target_data.h5\" , 'r')\n",
    "        for i in pad_file.keys():\n",
    "            if i == \"patch_size\":\n",
    "                continue\n",
    "            self.target[i] = pad_file[i][:]\n",
    "        \n",
    "    def pad_raw_data(self, patch_size, pad_path, raw_path):\n",
    "        # input\n",
    "        raw_data = None\n",
    "        raw_file = None\n",
    "        if self.check_file_exist(raw_path, \"raw_data_\"):\n",
    "            raw_data, raw_file = self.load_raw_data(raw_path)\n",
    "        else:\n",
    "            raw_data, raw_file = self.fetch_raw_data(raw_path)\n",
    "        \n",
    "        pad_data = {m : defaultdict(list) for m in self.moda}\n",
    "        for m in raw_data.keys():\n",
    "            data = defaultdict(list)\n",
    "            for i in raw_data[m].keys():\n",
    "                for j in range(raw_data[m][i].shape[0]):\n",
    "                    img = self.zero_pad(raw_data[m][i][j], patch_size)\n",
    "                    data[img.shape].append(img)\n",
    "            pad_data[m] = data\n",
    "            \n",
    "        for f in raw_file:\n",
    "            f.close()\n",
    "\n",
    "        for m in pad_data.keys():\n",
    "            with h5py.File(pad_path + \"pad_data_\" + m + \".h5\" , 'w') as f:\n",
    "                f.create_dataset(\"patch_size\", data=patch_size)\n",
    "                for i in pad_data[m]:\n",
    "                    f.create_dataset(str(i), data=pad_data[m][i])\n",
    "        \n",
    "        for m in pad_data.keys():\n",
    "            pad_file = h5py.File(pad_path + \"pad_data_\" + m + \".h5\" , 'r')\n",
    "            for i in pad_file.keys():\n",
    "                if i == \"patch_size\":\n",
    "                    continue\n",
    "                self.input[m][i] = pad_file[i][:]\n",
    "        \n",
    "    \n",
    "    def load_data(self, patch_size=(32, 32, 32), \n",
    "                  pad_path=\"./model/h5df_data/\", raw_path=\"./model/h5df_data/\", target_path=\"./model/h5df_data/\"):\n",
    "                  # pad_path=\"./model/h5df_data/pad_data.h5\", raw_path=\"./model/h5df_data/raw_data.h5\"):\n",
    "        padded = True\n",
    "        if self.check_file_exist(raw_path, \"pad_data_\"):\n",
    "            for m in self.moda:\n",
    "                pad_file = h5py.File(pad_path + \"pad_data_\" + m + \".h5\", 'r')\n",
    "                if np.all(pad_file[\"patch_size\"][:] == list(patch_size)):\n",
    "                    # self.data = pad_file[\"pad_data\"]\n",
    "                    for i in pad_file.keys():\n",
    "                        self.input[m][i] = pad_file[i][:]\n",
    "                else:\n",
    "                    padded = False\n",
    "                    break\n",
    "        if padded == False:\n",
    "            self.pad_raw_data(patch_size, pad_path, raw_path)\n",
    "        \n",
    "        if os.path.isfile(target_path + \"pad_target_data.h5\"):\n",
    "            pad_file = h5py.File(target_path + \"pad_target_data.h5\", 'r')\n",
    "            if np.all(pad_file[\"patch_size\"][:] == list(patch_size)):\n",
    "                # self.data = pad_file[\"pad_data\"]\n",
    "                for i in pad_file.keys():\n",
    "                    self.target[i] = pad_file[i][:]\n",
    "            else:\n",
    "                self.pad_target_data(patch_size, pad_path, target_path)\n",
    "        else:\n",
    "            self.pad_target_data(patch_size, pad_path, target_path)\n",
    "            \n",
    "    \n",
    "    def show_image(self, images):\n",
    "        # show image with [None, None, : ,: ,:] dimension\n",
    "        def show_frame(id):\n",
    "            length = len(images)\n",
    "            for i in range(length):\n",
    "                ax = plt.subplot(1, length, i+1)\n",
    "                if (i == 0):\n",
    "                    ax.set_title(\"Input\")\n",
    "                if (i == 1):\n",
    "                    ax.set_title(\"Target\")\n",
    "                if (i == 2):\n",
    "                    ax.set_title(\"Output\")\n",
    "                plt.imshow(images[i][0, 0, id, :, :], cmap='gray')\n",
    "        interact(show_frame, \n",
    "                 id=widgets.IntSlider(min=0, max=images[0].shape[2]-1, step=1, value=images[0].shape[2]/2))\n",
    "        \n",
    "        \n",
    "    def gen_patch_index(self, patch_size, patch_gap, index_path):\n",
    "        count = 0\n",
    "        patch_index = defaultdict(list)\n",
    "        # https://arxiv.org/pdf/1710.02316.pdf\n",
    "        # at least 0.01% voxels contain lesions\n",
    "        voxel = int(patch_size[0]*patch_size[1]*patch_size[2]*0.0001)\n",
    "        \n",
    "        for i in self.data:\n",
    "            if i == \"patch_size\":\n",
    "                continue\n",
    "            shape = self.data[i][0][0].shape\n",
    "            patch_num = [int((shape[i]-patch_size[i]) / patch_gap) for i in range(len(shape))]\n",
    "\n",
    "            for j in range(self.data[i].shape[0]):\n",
    "                patch_ind = []\n",
    "                # assume this is a 3d image\n",
    "                for a in range(patch_num[0]):\n",
    "                    for b in range(patch_num[1]):\n",
    "                        for c in range(patch_num[2]):\n",
    "                            patch_iter = [a * patch_gap, b * patch_gap, c * patch_gap, 1]\n",
    "                            if (np.sum(self.data[i][j][1][patch_iter[0]:patch_iter[0] + patch_size[0],\n",
    "                                                          patch_iter[1]:patch_iter[1] + patch_size[1],\n",
    "                                                          patch_iter[2]:patch_iter[2] + patch_size[2]]) <= voxel):\n",
    "                                # 0: does not satisfy, need to skip when generating\n",
    "                                patch_iter[3] = 0\n",
    "                            patch_ind.append(patch_iter)\n",
    "                patch_index[i].append(patch_ind)\n",
    "            \n",
    "            for c in range(len(patch_index[i])):\n",
    "                # in-place shuffle\n",
    "                np.random.shuffle(patch_index[i][c])\n",
    "\n",
    "            # total number of patches for this shape\n",
    "            count += len(patch_ind) * self.data[i].shape[0]\n",
    "        \n",
    "        with h5py.File(index_path, 'w') as f:\n",
    "            f.create_dataset(\"count\", data=count)\n",
    "            f.create_dataset(\"patch_size\", data=patch_size)\n",
    "            f.create_dataset(\"patch_gap\", data=patch_gap)\n",
    "            for i in patch_index:\n",
    "                f.create_dataset(str(i), data=patch_index[i])\n",
    "        \n",
    "        index_file = h5py.File(index_path, 'r')\n",
    "        for i in index_file.keys():\n",
    "            if i == \"count\" or i == \"patch_size\" or i == \"patch_gap\":\n",
    "                continue\n",
    "            self.patch_index[i] = index_file[i][:]\n",
    "        # return the total number of patches\n",
    "        return index_file[\"count\"][()]\n",
    "\n",
    "    def load_patch_index(self, patch_size, patch_gap, index_path):\n",
    "        if os.path.isfile(index_path):\n",
    "            index_file = h5py.File(index_path, 'r')\n",
    "            # print(list(pat_ind.keys()))\n",
    "            if (np.all(index_file[\"patch_size\"][:] == list(patch_size))) and (index_file[\"patch_gap\"][()] == patch_gap):\n",
    "                for i in index_file.keys():\n",
    "                    if i == \"count\" or i == \"patch_size\" or i == \"patch_gap\":\n",
    "                        continue\n",
    "                    self.patch_index[i] = index_file[i][:]\n",
    "                return index_file[\"count\"][()]\n",
    "            else:\n",
    "                index_file.close()\n",
    "                return self.gen_patch_index(patch_size, patch_gap, index_path)\n",
    "        else:\n",
    "            return self.gen_patch_index(patch_size, patch_gap, index_path)\n",
    "            \n",
    "    def prekfold(self, patch_size, patch_gap, batch_size, kfold=5, index_path='./model/h5df_data/pat_ind.h5'):\n",
    "        self.kfold = kfold\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_gap = patch_gap\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # initialize validation index for training\n",
    "        # K-fold LOOCV: leave one out cross validation\n",
    "        for i in self.data:\n",
    "            if i == \"patch_size\":\n",
    "                continue\n",
    "            self.valid_index[i] = random.sample(range(self.kfold), self.kfold)\n",
    "\n",
    "        num = self.load_patch_index(patch_size, patch_gap, index_path)\n",
    "        train_num = num // self.kfold * (self.kfold - 1)\n",
    "        valid_num = num - train_num\n",
    "        \n",
    "        return train_num // batch_size, valid_num // batch_size\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Data([\"FLAIR_preprocessed\", \"T1_preprocessed\", \"T2_preprocessed\"])\n",
    "\n",
    "# pad_raw_data(self, patch_size, pad_path, raw_path)\n",
    "# d.pad_raw_data((32, 32, 32), \"./model/h5df_data/\",  \"./model/h5df_data/\")\n",
    "# d.pad_target_data((32, 32, 32), \"./model/h5df_data/\",  \"./model/h5df_data/\")\n",
    "d.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# d = Data([\"FLAIR_preprocessed\", \"T1_preprocessed\", \"T2_preprocessed\"])\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "keys = [\"FLAIR_preprocessed\", \"T1_preprocessed\", \"T2_preprocessed\"]\n",
    "d = {key:defaultdict(list) for key in keys}\n",
    "# d = defaultdict(list)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
