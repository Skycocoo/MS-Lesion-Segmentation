{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# fold6_dice_multi_weights-02-0.38.hdf5\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "os.chdir(\"/scratch/yl4217/MS-Lesion-Segmentation/\")\n",
    "\n",
    "config = {}\n",
    "config[\"weights_file\"] = os.getcwd() + '/model/weight'\n",
    "config[\"modality\"] = [\"FLAIR_preprocessed\", \"T1_preprocessed\", \"T2_preprocessed\"]\n",
    "\n",
    "config[\"patch_size\"] = (64, 64, 64)  # switch to None to train on the whole image\n",
    "config[\"patch_gap\"] = 16\n",
    "config[\"batch_size\"] = 2\n",
    "config[\"kfold\"] = 5\n",
    "\n",
    "config[\"input_shape\"] = (len(config[\"modality\"]), None, None, None)\n",
    "config[\"depth\"] = 4 # depth of layers for V/Unet\n",
    "config[\"n_base_filters\"] = 32\n",
    "config[\"pool_size\"] = (2, 2, 2)  # pool size for the max pooling operations\n",
    "config[\"deconvolution\"] = True  # if False, will use upsampling instead of deconvolution\n",
    "\n",
    "config[\"patience\"] = 10  # learning rate will be reduced after this many epochs if the validation loss is not improving\n",
    "config[\"early_stop\"] = 10  # training will be stopped after this many epochs without the validation loss improving\n",
    "config[\"initial_learning_rate\"] = 0.0000001\n",
    "# config[\"initial_learning_rate\"] = 0.00001\n",
    "config[\"learning_rate_drop\"] = 0.5  # factor by which the learning rate will be reduced\n",
    "config[\"n_epochs\"] = 10\n",
    "\n",
    "\n",
    "from model.multi_data import *\n",
    "from model.multi_generator import *\n",
    "from model.modelDice import *\n",
    "from model.recon import *\n",
    "\n",
    "weight_path = ['/model/weight/fold6_dice_multi_weights-02-0.38.hdf5',\n",
    "              ]\n",
    "weight_name = ['dice',\n",
    "              ]\n",
    "\n",
    "\n",
    "d = Data(config[\"modality\"])\n",
    "d.load_data(config[\"patch_size\"])\n",
    "train_num, valid_num = d.prekfold(config[\"patch_size\"], config[\"patch_gap\"], config[\"batch_size\"], config[\"kfold\"])\n",
    "\n",
    "# train_generator = DataGenerator(d.moda, d.input, d.target, d.patch_index, d.kfold, d.batch_size,\n",
    "#                                 d.patch_size, d.patch_gap, d.valid_index, True)\n",
    "# valid_generator = DataGenerator(d.moda, d.input, d.target, d.patch_index, d.kfold, d.batch_size,\n",
    "#                                 d.patch_size, d.patch_gap, d.valid_index, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 256, 256)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(d.input.values()))['(128, 256, 256)'].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weight:  dice\n",
      "{'(128, 256, 256)': [1, 2, 0, 4, 3], '(192, 512, 512)': [1, 3, 4, 0, 2], '(320, 384, 384)': [4, 1, 2, 0, 3]}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-35b63c9dce3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mdir_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./model/h5df_data/recon/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mweight_name\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi_weight\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/yl4217/MS-Lesion-Segmentation/model/recon.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, patch, index)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# store patch data in self.data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mnew_data_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_and\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_data_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_data_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "for i_weight in range(len(weight_path)):\n",
    "    print(\"loading weight: \", weight_name[i_weight])\n",
    "    model = unet_model_3d(input_shape=config[\"input_shape\"],\n",
    "                                  pool_size=config[\"pool_size\"],\n",
    "                                  initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "                                  deconvolution=config[\"deconvolution\"],\n",
    "                                  depth=config[\"depth\"],\n",
    "                                  n_base_filters=config[\"n_base_filters\"])\n",
    "    model.load_weights(os.getcwd() + weight_path[i_weight]) \n",
    "    \n",
    "    print(d.valid_index)\n",
    "    fold_index = 0\n",
    "    for i in d.valid_index:\n",
    "        # i: shape\n",
    "        j = d.valid_index[i][fold_index]\n",
    "        sample_input = next(iter(d.input.values()))\n",
    "        output = Reconstruct(j, sample_input[i].shape[1:], config[\"patch_size\"], False)\n",
    "        image = Reconstruct(j, sample_input[i].shape[1:], config[\"patch_size\"], False)\n",
    "        target = Reconstruct(j, sample_input[i].shape[1:], config[\"patch_size\"], False)\n",
    "        for ind in range(d.patch_index[i][j].shape[0]):\n",
    "            \n",
    "            patch = d.patch_index[i][j][ind]\n",
    "            img = []\n",
    "            for m in d.moda:\n",
    "                data_image = d.input[m][i][j]\n",
    "                img.append(data_image[patch[0]:patch[0]+d.patch_size[0], \n",
    "                                 patch[1]:patch[1]+d.patch_size[1], \n",
    "                                 patch[2]:patch[2]+d.patch_size[2]])\n",
    "            \n",
    "            data_image = d.input[\"FLAIR_preprocessed\"][i][j]\n",
    "            image_i = np.expand_dims(data_image[patch[0]:patch[0]+d.patch_size[0], \n",
    "                                     patch[1]:patch[1]+d.patch_size[1], \n",
    "                                     patch[2]:patch[2]+d.patch_size[2]], axis=0)\n",
    "            \n",
    "            data_target = d.target[i][j]\n",
    "            target_i = data_target[patch[0]:patch[0]+d.patch_size[0], \n",
    "                             patch[1]:patch[1]+d.patch_size[1], \n",
    "                             patch[2]:patch[2]+d.patch_size[2]]\n",
    "            result = model.predict(np.array(np.expand_dims(img, axis=0)))\n",
    "            output.add(result, patch)\n",
    "            image.add(image_i, patch)\n",
    "            target.add(target_i, patch)\n",
    "            \n",
    "        dir_name = './model/h5df_data/recon/' + weight_name[i_weight] + '/'\n",
    "        os.makedirs(os.path.dirname(dir_name), exist_ok=True)\n",
    "        file_name = '/recon/' + weight_name[i_weight] + '/'+ str(d.data[i][j][0].shape)\n",
    "        output.store(file_name + \"_muti_uniform_output\")\n",
    "        image.store(file_name + \"_multi_input_flair\")\n",
    "        target.store(file_name + \"_multi_target\")\n",
    "\n",
    "print(\"finish reconstructing image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target {'[192 512 512]': <HDF5 file \"(192, 512, 512)_target.h5\" (mode r)>, '[320 384 384]': <HDF5 file \"(320, 384, 384)_target.h5\" (mode r)>, '[128 256 256]': <HDF5 file \"(128, 256, 256)_target.h5\" (mode r)>}\n",
      "uniform {'[128 256 256]': <HDF5 file \"(128, 256, 256)_uniform_output.h5\" (mode r)>, '[192 512 512]': <HDF5 file \"(192, 512, 512)_uniform_output.h5\" (mode r)>, '[320 384 384]': <HDF5 file \"(320, 384, 384)_uniform_output.h5\" (mode r)>}\n",
      "weight {}\n",
      "finish determining the optimal threshold\n",
      "finish restoring thresholded image\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def fetch_file():\n",
    "    path = os.getcwd() + '/model/h5df_data/recon/'\n",
    "    root, sub_dir, _ = next(os.walk(path))\n",
    "    total = {}\n",
    "    for sub in sub_dir:\n",
    "        _, _, sub_files = next(os.walk(root + sub))\n",
    "        uniform = {}\n",
    "        weight = {}\n",
    "        target = {}\n",
    "        for file in sub_files:\n",
    "            if \"nii\" not in file and \"threshold\" not in file:\n",
    "                cur_file = h5py.File(root + sub + '/' + file, 'r')\n",
    "                if \"weight\" in file:\n",
    "                    weight[str(cur_file[\"shape\"][()])]= cur_file\n",
    "                if \"uniform\" in file:\n",
    "                    uniform[str(cur_file[\"shape\"][()])]= cur_file\n",
    "                if \"target\" in file:\n",
    "                    target[str(cur_file[\"shape\"][()])]= cur_file\n",
    "        total[sub] = [target, uniform, weight]\n",
    "    return total[\"dice\"]\n",
    "\n",
    "def fetch_threshold_file():\n",
    "    path = os.getcwd() + '/model/h5df_data/recon/dice/'\n",
    "    root, sub_dir, _ = next(os.walk(path))\n",
    "    total = []\n",
    "    for sub in sub_dir:\n",
    "        _, _, sub_files = next(os.walk(root + sub))\n",
    "        for file in sub_files:\n",
    "            if \"nii.gz\" in file and \"threshold\" in file:\n",
    "                total.append(path + sub + '/' + file)\n",
    "    return total\n",
    "\n",
    "def dice(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = np.array(y_true).flatten()\n",
    "    y_pred_f = np.array(y_pred).flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    # tensorflow computation graph: will not configure print as one of the graph, unless using tf.Print()\n",
    "    return (2.*intersection+smooth) / (np.sum(y_true_f)+np.sum(y_pred_f)+smooth)\n",
    "\n",
    "def calc_thres(output, target):\n",
    "    it = np.arange(0, 1.01, 0.01)\n",
    "    dice_thre = []\n",
    "    for i in it:\n",
    "        dice_thre.append(dice(target, output>i))\n",
    "    return dice_thre\n",
    "\n",
    "\n",
    "total = fetch_file()\n",
    "table = {0: \"target\", 1: \"uniform\", 2: \"weight\"}\n",
    "\n",
    "for i in range(len(total)):\n",
    "    print(table[i], total[i])\n",
    "#     for j in range(len(total[i])):\n",
    "#         print(table[j])\n",
    "#         for k in total[i][j]:\n",
    "#             print(total[i][j][k])\n",
    "\n",
    "path = os.getcwd() + '/model/h5df_data/recon/'\n",
    "\n",
    "j = 1\n",
    "threshold = np.zeros(101)\n",
    "for i in total[0]:\n",
    "    threshold += calc_thres(total[1][i][\"data\"][:], total[0][i][\"data\"][:])\n",
    "threshold /= len(total[0])\n",
    "plt.plot(np.arange(0, 1.01, 0.01), threshold)\n",
    "opt_dice = threshold[np.argmax(threshold)]\n",
    "opt_thre = np.arange(0, 1.01, 0.01)[np.argmax(threshold)]\n",
    "plt.savefig(path + 'dice/' + table[j] + '_dice_' + str(opt_dice) + '_opt_' + str(opt_thre) + '_threshold.png')\n",
    "plt.clf()\n",
    "plt.cla()\n",
    "for i in total[0]:\n",
    "    nib.save(nib.Nifti1Image(np.array(total[1][i][\"data\"][:]>opt_thre).astype(int), np.eye(4)), path + 'dice/' + os.path.basename(total[1][i].filename) + \"_\" + table[j] + \"_\" + str(opt_thre) + \"_threshold.nii.gz\")\n",
    "        \n",
    "print(\"finish determining the optimal threshold\")\n",
    "\n",
    "threshold_total = fetch_threshold_file()\n",
    "for i in threshold_total:\n",
    "    image = nib.load(i)\n",
    "    nib.save(nib.Nifti1Image(image.get_fdata(), np.eye(4)), i)\n",
    "\n",
    "print(\"finish restoring thresholded image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
