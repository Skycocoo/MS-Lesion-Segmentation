{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "config = {}\n",
    "config[\"weights_file\"] = os.getcwd() + '/model/weight'\n",
    "config[\"patch_size\"] = (64, 64, 64)  # switch to None to train on the whole image\n",
    "config[\"patch_gap\"] = 16\n",
    "config[\"batch_size\"] = 2\n",
    "config[\"kfold\"] = 5\n",
    "\n",
    "config[\"input_shape\"] = (1, None, None, None)\n",
    "config[\"depth\"] = 4 # depth of layers for V/Unet\n",
    "config[\"n_base_filters\"] = 32\n",
    "config[\"pool_size\"] = (2, 2, 2)  # pool size for the max pooling operations\n",
    "config[\"deconvolution\"] = True  # if False, will use upsampling instead of deconvolution\n",
    "\n",
    "config[\"patience\"] = 10  # learning rate will be reduced after this many epochs if the validation loss is not improving\n",
    "config[\"early_stop\"] = 10  # training will be stopped after this many epochs without the validation loss improving\n",
    "config[\"initial_learning_rate\"] = 0.00001\n",
    "config[\"learning_rate_drop\"] = 0.5  # factor by which the learning rate will be reduced\n",
    "config[\"n_epochs\"] = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading weight:  dice\n",
      "{}\n",
      "finish reconstructing image\n"
     ]
    }
   ],
   "source": [
    "from model.recon import *\n",
    "from model.model import *\n",
    "from model.data import *\n",
    "\n",
    "# weight_path = ['/model/weight/spring2019/fold0_all_patch_weights-05-0.40.hdf5',\n",
    "#                '/model/weight/spring2019/fold0_weights-03-0.39.hdf5',\n",
    "#                '/model/weight/spring2019/fold_binary0weights-05-0.06.hdf5',\n",
    "#               ]\n",
    "# weight_name = ['all',\n",
    "#                'normal',\n",
    "#                'binary',\n",
    "#               ]\n",
    "\n",
    "weight_path = ['/model/weight/fold0_weights-03-0.38.hdf5',\n",
    "              ]\n",
    "weight_name = ['dice',\n",
    "              ]\n",
    "\n",
    "d = Data()\n",
    "d.load_data(config[\"patch_size\"])\n",
    "# set up valid index\n",
    "train_num, valid_num = d.prekfold(config[\"patch_size\"], config[\"patch_gap\"], config[\"batch_size\"], config[\"kfold\"])\n",
    "\n",
    "for i_weight in range(len(weight_path)):\n",
    "    print(\"loading weight: \", weight_name[i_weight])\n",
    "    model = unet_model_3d(input_shape=config[\"input_shape\"],\n",
    "                                  pool_size=config[\"pool_size\"],\n",
    "                                  initial_learning_rate=config[\"initial_learning_rate\"],\n",
    "                                  deconvolution=config[\"deconvolution\"],\n",
    "                                  depth=config[\"depth\"],\n",
    "                                  n_base_filters=config[\"n_base_filters\"])\n",
    "    model.load_weights(os.getcwd() + weight_path[i_weight]) \n",
    "    \n",
    "    print(d.valid_index)\n",
    "    fold_index = 0\n",
    "    for i in d.valid_index:\n",
    "        j = d.valid_index[i][fold_index]\n",
    "        normal = Reconstruct(j, d.data[i][j][0].shape, config[\"patch_size\"], False)\n",
    "        image = Reconstruct(j, d.data[i][j][0].shape, config[\"patch_size\"], False)\n",
    "        target = Reconstruct(j, d.data[i][j][0].shape, config[\"patch_size\"], False)\n",
    "        for ind in range(d.patch_index[i][j].shape[0]):\n",
    "            index = d.patch_index[i][j][ind]\n",
    "            image_i = np.expand_dims(d.data[i][j][0][\n",
    "                             index[0]:index[0]+d.patch_size[0], \n",
    "                             index[1]:index[1]+d.patch_size[1], \n",
    "                             index[2]:index[2]+d.patch_size[2]], axis=0)\n",
    "            target_i = np.expand_dims(d.data[i][j][1][\n",
    "                             index[0]:index[0]+d.patch_size[0], \n",
    "                             index[1]:index[1]+d.patch_size[1], \n",
    "                             index[2]:index[2]+d.patch_size[2]], axis=0)\n",
    "            result = model.predict([image_i[None, :]])\n",
    "            normal.add(result, index)\n",
    "            image.add(image_i, index)\n",
    "            target.add(target_i, index)\n",
    "        dir_name = './model/h5df_data/recon/' + weight_name[i_weight] + '/'\n",
    "        os.makedirs(os.path.dirname(dir_name), exist_ok=True)\n",
    "        file_name = '/recon/' + weight_name[i_weight] + '/'+ str(d.data[i][j][0].shape)\n",
    "        normal.store(file_name + \"_uniform_output\")\n",
    "        image.store(file_name + \"_input\")\n",
    "        target.store(file_name + \"_target\")\n",
    "\n",
    "print(\"finish reconstructing image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all\n",
      "target\n",
      "<HDF5 file \"(192, 512, 512)_target.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_target.h5\" (mode r)>\n",
      "<HDF5 file \"(128, 256, 256)_target.h5\" (mode r)>\n",
      "uniform\n",
      "<HDF5 file \"(192, 512, 512)_uniform_output.h5\" (mode r)>\n",
      "<HDF5 file \"(128, 256, 256)_uniform_output.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_uniform_output.h5\" (mode r)>\n",
      "weight\n",
      "<HDF5 file \"(128, 256, 256)_weighted_output.h5\" (mode r)>\n",
      "<HDF5 file \"(192, 512, 512)_weighted_output.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_weighted_output.h5\" (mode r)>\n",
      "normal\n",
      "target\n",
      "<HDF5 file \"(192, 512, 512)_target.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_target.h5\" (mode r)>\n",
      "<HDF5 file \"(128, 256, 256)_target.h5\" (mode r)>\n",
      "uniform\n",
      "<HDF5 file \"(192, 512, 512)_uniform_output.h5\" (mode r)>\n",
      "<HDF5 file \"(128, 256, 256)_uniform_output.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_uniform_output.h5\" (mode r)>\n",
      "weight\n",
      "<HDF5 file \"(128, 256, 256)_weighted_output.h5\" (mode r)>\n",
      "<HDF5 file \"(192, 512, 512)_weighted_output.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_weighted_output.h5\" (mode r)>\n",
      "binary\n",
      "target\n",
      "<HDF5 file \"(192, 512, 512)_target.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_target.h5\" (mode r)>\n",
      "<HDF5 file \"(128, 256, 256)_target.h5\" (mode r)>\n",
      "uniform\n",
      "<HDF5 file \"(192, 512, 512)_uniform_output.h5\" (mode r)>\n",
      "<HDF5 file \"(128, 256, 256)_uniform_output.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_uniform_output.h5\" (mode r)>\n",
      "weight\n",
      "<HDF5 file \"(128, 256, 256)_weighted_output.h5\" (mode r)>\n",
      "<HDF5 file \"(192, 512, 512)_weighted_output.h5\" (mode r)>\n",
      "<HDF5 file \"(320, 384, 384)_weighted_output.h5\" (mode r)>\n"
     ]
    }
   ],
   "source": [
    "table = {0: \"target\", 1: \"uniform\", 2: \"weight\"}\n",
    "\n",
    "for i in total:\n",
    "    print(i)\n",
    "    for j in range(len(total[i])):\n",
    "        print(table[j])\n",
    "        for k in total[i][j]:\n",
    "            print(total[i][j][k])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<HDF5 file \"(128, 256, 256)_target.h5\" (mode r)>\n",
      "(128, 256, 256)_target.h5\n"
     ]
    }
   ],
   "source": [
    "for i in target:\n",
    "    for j in range(len(target[i])):\n",
    "        print(target[i][j])\n",
    "        print(os.path.basename(target[i][j].filename))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0, 1.01, 0.01).shape\n",
    "np.zeros(101).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(it, dice_thre)\n",
    "plt.savefig()\n",
    "print(it[np.argmax(dice_thre)], dice_thre[np.argmax(dice_thre)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/scratch/yl4217/MS-Lesion-Segmentation/model/h5df_data/recon/multi/(128, 256, 256)_uniform_output.h5_uniform_0.54_threshold.nii.gz', '/scratch/yl4217/MS-Lesion-Segmentation/model/h5df_data/recon/multi/(192, 512, 512)_uniform_output.h5_uniform_0.54_threshold.nii.gz', '/scratch/yl4217/MS-Lesion-Segmentation/model/h5df_data/recon/multi/(320, 384, 384)_uniform_output.h5_uniform_0.54_threshold.nii.gz']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def fetch_file():\n",
    "    path = os.getcwd() + '/model/h5df_data/recon/'\n",
    "    root, sub_dir, _ = next(os.walk(path))\n",
    "    total = {}\n",
    "#     uniform = defaultdict(list)\n",
    "#     weight = defaultdict(list)\n",
    "#     target = defaultdict(list)\n",
    "    for sub in sub_dir:\n",
    "        if \"ipynb_checkpoints\" in sub:\n",
    "            continue\n",
    "        _, _, sub_files = next(os.walk(root + sub))\n",
    "        ar = []\n",
    "        for file in sub_files:\n",
    "#             print(file)\n",
    "            if \"nii.gz\" in file and \"threshold\" in file:\n",
    "                ar.append(path+sub+'/'+file)\n",
    "        total[sub] = ar\n",
    "    return total[\"multi\"]\n",
    "\n",
    "def dice(y_true, y_pred, smooth=1.):\n",
    "    y_true_f = np.array(y_true).flatten()\n",
    "    y_pred_f = np.array(y_pred).flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    # tensorflow computation graph: will not configure print as one of the graph, unless using tf.Print()\n",
    "    return (2.*intersection+smooth) / (np.sum(y_true_f)+np.sum(y_pred_f)+smooth)\n",
    "# print(dice(merge_target, merge_result>0.02))\n",
    "            \n",
    "total = fetch_file()\n",
    "print(total)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "raw_data = defaultdict(list)\n",
    "# raw_data = []\n",
    "# raw_data[i][0]: image, raw_data[i][1]: target\n",
    "for i in total:\n",
    "    image = nib.load(i)\n",
    "    nib.save(nib.Nifti1Image(image.get_fdata(), np.eye(4)), i)\n",
    "#     for j in range(len(total[i])):\n",
    "#         image = nib.load(total[i][j])\n",
    "#         nib.save(nib.Nifti1Image(image.get_fdata(), np.eye(4)), total[i][j])\n",
    "# #         raw_data[str(image.shape)].append(image.get_fdata())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'(192, 512, 512)': [(192, 512, 512), (192, 512, 512)], '(320, 384, 384)': [(320, 384, 384), (320, 384, 384)], '(128, 256, 256)': [(128, 256, 256), (128, 256, 256)]})\n"
     ]
    }
   ],
   "source": [
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ca36c97a3b4acdbf5e476d31fb0f37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=96, description='id', max=191), Output()), _dom_classes=('widget-interac…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_image(images):\n",
    "    # show image with [None, None, : ,: ,:] dimension\n",
    "    def show_frame(id):\n",
    "        length = len(images)\n",
    "        for i in range(length):\n",
    "            ax = plt.subplot(1, length, i+1)\n",
    "            if (i == 0):\n",
    "                ax.set_title(\"Input\")\n",
    "            if (i == 1):\n",
    "                ax.set_title(\"Target\")\n",
    "            if (i == 2):\n",
    "                ax.set_title(\"Output\")\n",
    "            plt.imshow(images[i][id, :, :], cmap='gray')\n",
    "    interact(show_frame, \n",
    "             id=widgets.IntSlider(min=0, max=images[0].shape[0]-1, step=1, value=images[0].shape[0]/2))\n",
    "show_image(raw_data['(192, 512, 512)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
